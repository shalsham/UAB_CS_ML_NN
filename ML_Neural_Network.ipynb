{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eae14155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, matplotlib.pyplot as plt, pandas as pd, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "397ca449",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d92b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self, n_in, n_out, *, actv):\n",
    "        if actv == \"relu\": self.W = rng.normal(0, 2/np.sqrt(n_in), size=(n_out, 1+n_in))\n",
    "        else: self.W = rng.uniform(-(x := np.sqrt(6/(n_in+n_out))), x, size=(n_out, 1+n_in))\n",
    "        #self.W = rng.uniform(-0.5, 0.5, size=(n_out, 1+n_in))\n",
    "        self.actv_func = getattr(self, f\"{actv}_func\")\n",
    "        self.actv_grad = getattr(self, f\"{actv}_grad\")\n",
    "        self.X = None\n",
    "        self.Z = None\n",
    "        self.A = None\n",
    "        self.d_Z = None\n",
    "        self.d_AZ = None\n",
    "        self.Z_exp = None\n",
    "        self.s = None\n",
    "        self.size = 0\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.W @ np.vstack((np.ones((1, len(X.T)), dtype=np.float64), X))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        n = len(X.T)\n",
    "        if n > self.size:\n",
    "            self.X = np.vstack((np.ones((1, n), dtype=np.float64), X))\n",
    "            self.Z = self.W @ self.X\n",
    "            self.A = np.zeros(self.Z.shape, dtype=np.float64)\n",
    "            self.d_Z = np.zeros(self.Z.shape, dtype=np.float64)\n",
    "            self.d_W = np.zeros(self.W.shape, dtype=np.float64)\n",
    "            self.d_A = np.zeros(self.Z.shape, dtype=np.float64)\n",
    "            self.Z_exp = np.zeros(self.Z.shape, dtype=np.float64)\n",
    "            self.d_AZ = np.zeros((len(self.Z), len(self.Z)), dtype=np.float64)\n",
    "            self.s = np.zeros((len(self.Z.T), ), dtype=np.float64)\n",
    "            self.size = n\n",
    "        else:\n",
    "            self.X[1:, :n] = X\n",
    "            np.dot(self.W, self.X[:, :n], out=self.Z[:, :n])\n",
    "        self.actv_func(n)\n",
    "        return self.A[:, :n]\n",
    "    \n",
    "    def backward(self, d_A):\n",
    "        n = len(d_A.T)\n",
    "        self.d_A[:, :n] = d_A\n",
    "        self.actv_grad(n)\n",
    "        np.dot(self.d_Z[:, :n], self.X.T[:n, :], out=self.d_W)\n",
    "        return self.W.T[1:] @ self.d_Z[:, :n]\n",
    "    \n",
    "    def relu_func(self, size):\n",
    "        self.A[:, :size] = self.Z[:, :size] * (self.Z[:, :size] > 0)\n",
    "    \n",
    "    def relu_grad(self, size):\n",
    "        self.d_Z[:, :size] = self.d_A[:, :size] * (self.Z[:, :size] > 0)\n",
    "    \n",
    "    def softmax_func(self, size):\n",
    "        np.exp(np.maximum(-100, np.minimum(100, self.Z[:, :size])), out=self.Z_exp[:, :size])\n",
    "        np.divide(self.Z_exp[:, :size], self.Z_exp[:, :size].sum(axis=0), out=self.A[:, :size])\n",
    "    \n",
    "    def softmax_grad(self, size):\n",
    "        self.d_Z[:, :size] = self.d_A[:, :size] * self.A[:, :size]\n",
    "        '''\n",
    "        self.s[:size] = self.Z_exp[:, :size].sum(axis=0)\n",
    "        for j, z_exp in enumerate(self.Z_exp.T[:size, :]):\n",
    "            for i in range(len(z_exp)):\n",
    "                self.d_AZ[i, i] = (self.s[i] - self.Z_exp[i, j]) * self.Z_exp[i, j]\n",
    "                for i1 in range(i+1, len(z_exp)):\n",
    "                    self.d_AZ[i, i1] = self.d_AZ[i1, i] = - (self.Z_exp[i, j] * self.Z_exp[i1, j])\n",
    "            self.d_AZ /= self.s[j] ** 2\n",
    "            self.d_Z[:, j] = self.d_AZ[:, :size] @ self.d_A.T[j]\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52a22a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuNet:\n",
    "    \n",
    "    def __init__(self, n_in, ns_out, *, actvs=None, alpha=None, beta=2**-4, tol=0.15, max_iter=256):\n",
    "        n = len(ns_out)\n",
    "        in_out = [n_in] + ns_out\n",
    "        if actvs is None: actvs = ['relu'] * (n - 1) + ['softmax']\n",
    "        self.layers = [Layer(in_out[i], in_out[i+1], actv=actv) for i, actv in enumerate(actvs)]\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        if not isinstance(X, np.ndarray): X = np.array(X)\n",
    "        y_p = X = np.atleast_2d(X).T\n",
    "        for layer in self.layers: y_p = layer(y_p)\n",
    "        return np.apply_along_axis(np.argmax, 0, y_p)\n",
    "    \n",
    "    @property\n",
    "    def W(self):\n",
    "        return np.array([layer.W for layer in self.layers])\n",
    "    \n",
    "    def one_hot(self, x, maxx=10):\n",
    "        return np.apply_along_axis(lambda k: (np.arange(maxx)==k).astype(int), 0, np.atleast_2d(x))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if not isinstance(X, np.ndarray): X = np.array(X)\n",
    "        if not isinstance(y, np.ndarray): y = np.array(y)\n",
    "        X = np.atleast_2d(X).T\n",
    "        y_hot = self.one_hot(y)\n",
    "        if self.alpha is not None: alpha, d_alpha = self.alpha, 0\n",
    "        else: alpha, d_alpha = 1, (1 - self.beta) / self.max_iter\n",
    "        for itr in range(self.max_iter):\n",
    "            y_p = X\n",
    "            for layer in self.layers: y_p = layer.forward(y_p)\n",
    "            d_y = (y_p - y_hot) / len(y)\n",
    "            for layer in reversed(self.layers):\n",
    "                d_y = layer.backward(d_y)\n",
    "                layer.W -= alpha * layer.d_W\n",
    "            alpha -= d_alpha\n",
    "            loss = np.sum((y_hot - y_p) ** 2, axis=0).mean()\n",
    "            acc = np.mean(np.apply_along_axis(np.argmax, 0, y_p) == y)\n",
    "            print(f\"{itr = }\\t{loss = :.6f}\\tacc = {100*acc:.3f}%\".expandtabs(10))\n",
    "            if loss < self.tol: break\n",
    "        return self\n",
    "    \n",
    "    def __repr__(self): return f\"NeuNet({len(self.layers)} layers)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd94b97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a8e3905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5e41385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42000, 784), (42000,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df.to_numpy().astype(np.float64)\n",
    "X = data.T[1:].T / 256\n",
    "y = data.T[0]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1428dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn = X[:30_000]\n",
    "y_trn = y[:30_000]\n",
    "X_tst = X[30_000:]\n",
    "y_tst = y[30_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a5d1332",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuNet(len(X.T), [16, 10], beta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88a9e7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr = 0   loss = 0.933918     acc = 8.960%\n",
      "itr = 1   loss = 0.914784     acc = 12.660%\n",
      "itr = 2   loss = 0.901289     acc = 16.323%\n",
      "itr = 3   loss = 0.888645     acc = 19.613%\n",
      "itr = 4   loss = 0.875556     acc = 22.167%\n",
      "itr = 5   loss = 0.861685     acc = 23.547%\n",
      "itr = 6   loss = 0.847793     acc = 24.530%\n",
      "itr = 7   loss = 0.834582     acc = 25.603%\n",
      "itr = 8   loss = 0.821753     acc = 26.903%\n",
      "itr = 9   loss = 0.809350     acc = 28.027%\n",
      "itr = 10  loss = 0.797340     acc = 29.297%\n",
      "itr = 11  loss = 0.785339     acc = 30.720%\n",
      "itr = 12  loss = 0.773219     acc = 32.543%\n",
      "itr = 13  loss = 0.760698     acc = 34.810%\n",
      "itr = 14  loss = 0.747783     acc = 37.363%\n",
      "itr = 15  loss = 0.734292     acc = 40.037%\n",
      "itr = 16  loss = 0.719831     acc = 42.397%\n",
      "itr = 17  loss = 0.704378     acc = 44.657%\n",
      "itr = 18  loss = 0.687869     acc = 46.550%\n",
      "itr = 19  loss = 0.670716     acc = 48.430%\n",
      "itr = 20  loss = 0.652873     acc = 50.107%\n",
      "itr = 21  loss = 0.635256     acc = 51.373%\n",
      "itr = 22  loss = 0.617596     acc = 52.490%\n",
      "itr = 23  loss = 0.600985     acc = 53.800%\n",
      "itr = 24  loss = 0.584634     acc = 54.783%\n",
      "itr = 25  loss = 0.569783     acc = 55.763%\n",
      "itr = 26  loss = 0.555911     acc = 56.577%\n",
      "itr = 27  loss = 0.544104     acc = 57.247%\n",
      "itr = 28  loss = 0.532560     acc = 57.820%\n",
      "itr = 29  loss = 0.521973     acc = 58.397%\n",
      "itr = 30  loss = 0.510992     acc = 58.913%\n",
      "itr = 31  loss = 0.500688     acc = 59.550%\n",
      "itr = 32  loss = 0.489630     acc = 60.290%\n",
      "itr = 33  loss = 0.479480     acc = 61.343%\n",
      "itr = 34  loss = 0.468891     acc = 62.343%\n",
      "itr = 35  loss = 0.460773     acc = 63.407%\n",
      "itr = 36  loss = 0.453928     acc = 64.120%\n",
      "itr = 37  loss = 0.449737     acc = 64.453%\n",
      "itr = 38  loss = 0.447539     acc = 64.797%\n",
      "itr = 39  loss = 0.442578     acc = 64.900%\n",
      "itr = 40  loss = 0.440024     acc = 66.150%\n",
      "itr = 41  loss = 0.423314     acc = 67.187%\n",
      "itr = 42  loss = 0.406145     acc = 69.637%\n",
      "itr = 43  loss = 0.390437     acc = 70.877%\n",
      "itr = 44  loss = 0.375609     acc = 72.020%\n",
      "itr = 45  loss = 0.361601     acc = 73.917%\n",
      "itr = 46  loss = 0.350442     acc = 74.400%\n",
      "itr = 47  loss = 0.338997     acc = 76.463%\n",
      "itr = 48  loss = 0.334259     acc = 76.333%\n",
      "itr = 49  loss = 0.307591     acc = 79.020%\n",
      "itr = 50  loss = 0.303807     acc = 78.890%\n",
      "itr = 51  loss = 0.288951     acc = 79.830%\n",
      "itr = 52  loss = 0.300035     acc = 78.803%\n",
      "itr = 53  loss = 0.293639     acc = 78.840%\n",
      "itr = 54  loss = 0.284360     acc = 80.097%\n",
      "itr = 55  loss = 0.296288     acc = 78.703%\n",
      "itr = 56  loss = 0.327978     acc = 76.240%\n",
      "itr = 57  loss = 0.325418     acc = 76.440%\n",
      "itr = 58  loss = 0.329808     acc = 76.137%\n",
      "itr = 59  loss = 0.270256     acc = 81.500%\n",
      "itr = 60  loss = 0.243899     acc = 83.173%\n",
      "itr = 61  loss = 0.235316     acc = 84.400%\n",
      "itr = 62  loss = 0.234041     acc = 83.727%\n",
      "itr = 63  loss = 0.232801     acc = 84.437%\n",
      "itr = 64  loss = 0.238584     acc = 83.267%\n",
      "itr = 65  loss = 0.239099     acc = 83.897%\n",
      "itr = 66  loss = 0.250401     acc = 82.343%\n",
      "itr = 67  loss = 0.264227     acc = 81.533%\n",
      "itr = 68  loss = 0.262851     acc = 81.393%\n",
      "itr = 69  loss = 0.271626     acc = 81.090%\n",
      "itr = 70  loss = 0.275796     acc = 80.480%\n",
      "itr = 71  loss = 0.242187     acc = 83.493%\n",
      "itr = 72  loss = 0.227759     acc = 84.167%\n",
      "itr = 73  loss = 0.223333     acc = 84.987%\n",
      "itr = 74  loss = 0.216820     acc = 84.990%\n",
      "itr = 75  loss = 0.220338     acc = 85.133%\n",
      "itr = 76  loss = 0.221903     acc = 84.660%\n",
      "itr = 77  loss = 0.227763     acc = 84.437%\n",
      "itr = 78  loss = 0.247255     acc = 82.490%\n",
      "itr = 79  loss = 0.239404     acc = 83.130%\n",
      "itr = 80  loss = 0.235216     acc = 83.583%\n",
      "itr = 81  loss = 0.226082     acc = 84.373%\n",
      "itr = 82  loss = 0.220430     acc = 84.697%\n",
      "itr = 83  loss = 0.216697     acc = 85.010%\n",
      "itr = 84  loss = 0.218476     acc = 84.900%\n",
      "itr = 85  loss = 0.210953     acc = 85.483%\n",
      "itr = 86  loss = 0.209424     acc = 85.593%\n",
      "itr = 87  loss = 0.204948     acc = 85.983%\n",
      "itr = 88  loss = 0.206386     acc = 85.803%\n",
      "itr = 89  loss = 0.202298     acc = 86.210%\n",
      "itr = 90  loss = 0.204044     acc = 85.977%\n",
      "itr = 91  loss = 0.199843     acc = 86.420%\n",
      "itr = 92  loss = 0.201582     acc = 86.157%\n",
      "itr = 93  loss = 0.197493     acc = 86.593%\n",
      "itr = 94  loss = 0.199293     acc = 86.300%\n",
      "itr = 95  loss = 0.195271     acc = 86.770%\n",
      "itr = 96  loss = 0.197096     acc = 86.490%\n",
      "itr = 97  loss = 0.193121     acc = 86.907%\n",
      "itr = 98  loss = 0.194934     acc = 86.653%\n",
      "itr = 99  loss = 0.191008     acc = 87.083%\n",
      "itr = 100 loss = 0.192799     acc = 86.817%\n",
      "itr = 101 loss = 0.188944     acc = 87.227%\n",
      "itr = 102 loss = 0.190704     acc = 86.997%\n",
      "itr = 103 loss = 0.186951     acc = 87.407%\n",
      "itr = 104 loss = 0.188684     acc = 87.123%\n",
      "itr = 105 loss = 0.185055     acc = 87.530%\n",
      "itr = 106 loss = 0.186742     acc = 87.267%\n",
      "itr = 107 loss = 0.183264     acc = 87.650%\n",
      "itr = 108 loss = 0.184900     acc = 87.390%\n",
      "itr = 109 loss = 0.181595     acc = 87.720%\n",
      "itr = 110 loss = 0.183166     acc = 87.490%\n",
      "itr = 111 loss = 0.180030     acc = 87.893%\n",
      "itr = 112 loss = 0.181543     acc = 87.590%\n",
      "itr = 113 loss = 0.178559     acc = 87.963%\n",
      "itr = 114 loss = 0.179986     acc = 87.670%\n",
      "itr = 115 loss = 0.177171     acc = 88.047%\n",
      "itr = 116 loss = 0.178511     acc = 87.783%\n",
      "itr = 117 loss = 0.175860     acc = 88.090%\n",
      "itr = 118 loss = 0.177094     acc = 87.890%\n",
      "itr = 119 loss = 0.174611     acc = 88.170%\n",
      "itr = 120 loss = 0.175743     acc = 87.973%\n",
      "itr = 121 loss = 0.173435     acc = 88.243%\n",
      "itr = 122 loss = 0.174458     acc = 88.063%\n",
      "itr = 123 loss = 0.172321     acc = 88.307%\n",
      "itr = 124 loss = 0.173235     acc = 88.180%\n",
      "itr = 125 loss = 0.171272     acc = 88.347%\n",
      "itr = 126 loss = 0.172074     acc = 88.273%\n",
      "itr = 127 loss = 0.170280     acc = 88.387%\n",
      "itr = 128 loss = 0.170973     acc = 88.353%\n",
      "itr = 129 loss = 0.169354     acc = 88.497%\n",
      "itr = 130 loss = 0.169941     acc = 88.443%\n",
      "itr = 131 loss = 0.168490     acc = 88.577%\n",
      "itr = 132 loss = 0.168977     acc = 88.547%\n",
      "itr = 133 loss = 0.167688     acc = 88.650%\n",
      "itr = 134 loss = 0.168077     acc = 88.597%\n",
      "itr = 135 loss = 0.166941     acc = 88.683%\n",
      "itr = 136 loss = 0.167242     acc = 88.663%\n",
      "itr = 137 loss = 0.166257     acc = 88.723%\n",
      "itr = 138 loss = 0.166476     acc = 88.760%\n",
      "itr = 139 loss = 0.165630     acc = 88.767%\n",
      "itr = 140 loss = 0.165776     acc = 88.777%\n",
      "itr = 141 loss = 0.165054     acc = 88.790%\n",
      "itr = 142 loss = 0.165136     acc = 88.830%\n",
      "itr = 143 loss = 0.164527     acc = 88.863%\n",
      "itr = 144 loss = 0.164556     acc = 88.877%\n",
      "itr = 145 loss = 0.164046     acc = 88.920%\n",
      "itr = 146 loss = 0.164028     acc = 88.913%\n",
      "itr = 147 loss = 0.163601     acc = 88.943%\n",
      "itr = 148 loss = 0.163545     acc = 88.957%\n",
      "itr = 149 loss = 0.163184     acc = 88.973%\n",
      "itr = 150 loss = 0.163097     acc = 88.947%\n",
      "itr = 151 loss = 0.162792     acc = 88.987%\n",
      "itr = 152 loss = 0.162681     acc = 88.963%\n",
      "itr = 153 loss = 0.162422     acc = 89.007%\n",
      "itr = 154 loss = 0.162293     acc = 89.007%\n",
      "itr = 155 loss = 0.162067     acc = 89.040%\n",
      "itr = 156 loss = 0.161927     acc = 89.047%\n",
      "itr = 157 loss = 0.161725     acc = 89.087%\n",
      "itr = 158 loss = 0.161578     acc = 89.083%\n",
      "itr = 159 loss = 0.161395     acc = 89.100%\n",
      "itr = 160 loss = 0.161246     acc = 89.100%\n",
      "itr = 161 loss = 0.161077     acc = 89.117%\n",
      "itr = 162 loss = 0.160928     acc = 89.133%\n",
      "itr = 163 loss = 0.160769     acc = 89.143%\n",
      "itr = 164 loss = 0.160623     acc = 89.147%\n",
      "itr = 165 loss = 0.160472     acc = 89.147%\n",
      "itr = 166 loss = 0.160330     acc = 89.160%\n",
      "itr = 167 loss = 0.160187     acc = 89.167%\n",
      "itr = 168 loss = 0.160047     acc = 89.183%\n",
      "itr = 169 loss = 0.159910     acc = 89.200%\n",
      "itr = 170 loss = 0.159777     acc = 89.217%\n",
      "itr = 171 loss = 0.159645     acc = 89.217%\n",
      "itr = 172 loss = 0.159515     acc = 89.223%\n",
      "itr = 173 loss = 0.159388     acc = 89.223%\n",
      "itr = 174 loss = 0.159263     acc = 89.230%\n",
      "itr = 175 loss = 0.159141     acc = 89.227%\n",
      "itr = 176 loss = 0.159020     acc = 89.233%\n",
      "itr = 177 loss = 0.158902     acc = 89.227%\n",
      "itr = 178 loss = 0.158786     acc = 89.243%\n",
      "itr = 179 loss = 0.158674     acc = 89.247%\n",
      "itr = 180 loss = 0.158561     acc = 89.247%\n",
      "itr = 181 loss = 0.158452     acc = 89.277%\n",
      "itr = 182 loss = 0.158345     acc = 89.283%\n",
      "itr = 183 loss = 0.158241     acc = 89.290%\n",
      "itr = 184 loss = 0.158138     acc = 89.297%\n",
      "itr = 185 loss = 0.158037     acc = 89.297%\n",
      "itr = 186 loss = 0.157938     acc = 89.300%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr = 187 loss = 0.157840     acc = 89.293%\n",
      "itr = 188 loss = 0.157744     acc = 89.300%\n",
      "itr = 189 loss = 0.157650     acc = 89.307%\n",
      "itr = 190 loss = 0.157557     acc = 89.327%\n",
      "itr = 191 loss = 0.157466     acc = 89.340%\n",
      "itr = 192 loss = 0.157377     acc = 89.340%\n",
      "itr = 193 loss = 0.157288     acc = 89.350%\n",
      "itr = 194 loss = 0.157201     acc = 89.353%\n",
      "itr = 195 loss = 0.157117     acc = 89.350%\n",
      "itr = 196 loss = 0.157034     acc = 89.350%\n",
      "itr = 197 loss = 0.156952     acc = 89.360%\n",
      "itr = 198 loss = 0.156872     acc = 89.347%\n",
      "itr = 199 loss = 0.156793     acc = 89.347%\n",
      "itr = 200 loss = 0.156716     acc = 89.357%\n",
      "itr = 201 loss = 0.156639     acc = 89.357%\n",
      "itr = 202 loss = 0.156564     acc = 89.353%\n",
      "itr = 203 loss = 0.156490     acc = 89.350%\n",
      "itr = 204 loss = 0.156417     acc = 89.367%\n",
      "itr = 205 loss = 0.156346     acc = 89.370%\n",
      "itr = 206 loss = 0.156275     acc = 89.377%\n",
      "itr = 207 loss = 0.156206     acc = 89.380%\n",
      "itr = 208 loss = 0.156138     acc = 89.383%\n",
      "itr = 209 loss = 0.156071     acc = 89.383%\n",
      "itr = 210 loss = 0.156006     acc = 89.383%\n",
      "itr = 211 loss = 0.155941     acc = 89.383%\n",
      "itr = 212 loss = 0.155878     acc = 89.383%\n",
      "itr = 213 loss = 0.155815     acc = 89.380%\n",
      "itr = 214 loss = 0.155754     acc = 89.377%\n",
      "itr = 215 loss = 0.155694     acc = 89.380%\n",
      "itr = 216 loss = 0.155635     acc = 89.387%\n",
      "itr = 217 loss = 0.155577     acc = 89.387%\n",
      "itr = 218 loss = 0.155520     acc = 89.403%\n",
      "itr = 219 loss = 0.155464     acc = 89.410%\n",
      "itr = 220 loss = 0.155409     acc = 89.410%\n",
      "itr = 221 loss = 0.155355     acc = 89.413%\n",
      "itr = 222 loss = 0.155302     acc = 89.417%\n",
      "itr = 223 loss = 0.155250     acc = 89.420%\n",
      "itr = 224 loss = 0.155200     acc = 89.423%\n",
      "itr = 225 loss = 0.155150     acc = 89.420%\n",
      "itr = 226 loss = 0.155101     acc = 89.423%\n",
      "itr = 227 loss = 0.155053     acc = 89.423%\n",
      "itr = 228 loss = 0.155007     acc = 89.430%\n",
      "itr = 229 loss = 0.154961     acc = 89.437%\n",
      "itr = 230 loss = 0.154916     acc = 89.437%\n",
      "itr = 231 loss = 0.154873     acc = 89.433%\n",
      "itr = 232 loss = 0.154829     acc = 89.440%\n",
      "itr = 233 loss = 0.154787     acc = 89.440%\n",
      "itr = 234 loss = 0.154746     acc = 89.443%\n",
      "itr = 235 loss = 0.154706     acc = 89.453%\n",
      "itr = 236 loss = 0.154667     acc = 89.457%\n",
      "itr = 237 loss = 0.154630     acc = 89.460%\n",
      "itr = 238 loss = 0.154592     acc = 89.460%\n",
      "itr = 239 loss = 0.154555     acc = 89.460%\n",
      "itr = 240 loss = 0.154520     acc = 89.467%\n",
      "itr = 241 loss = 0.154485     acc = 89.477%\n",
      "itr = 242 loss = 0.154451     acc = 89.480%\n",
      "itr = 243 loss = 0.154418     acc = 89.487%\n",
      "itr = 244 loss = 0.154386     acc = 89.487%\n",
      "itr = 245 loss = 0.154355     acc = 89.487%\n",
      "itr = 246 loss = 0.154324     acc = 89.493%\n",
      "itr = 247 loss = 0.154295     acc = 89.493%\n",
      "itr = 248 loss = 0.154266     acc = 89.493%\n",
      "itr = 249 loss = 0.154238     acc = 89.497%\n",
      "itr = 250 loss = 0.154211     acc = 89.497%\n",
      "itr = 251 loss = 0.154185     acc = 89.503%\n",
      "itr = 252 loss = 0.154159     acc = 89.510%\n",
      "itr = 253 loss = 0.154135     acc = 89.510%\n",
      "itr = 254 loss = 0.154111     acc = 89.510%\n",
      "itr = 255 loss = 0.154088     acc = 89.513%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuNet(2 layers)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_trn, y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7eb38e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8885"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(model(X_tst)==y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbb5d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
